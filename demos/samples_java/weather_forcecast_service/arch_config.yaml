version: v0.1.0

listeners:
  ingress_traffic:
    address: 0.0.0.0
    port: 10000
    message_format: openai
    timeout: 30s

# Centralized way to manage LLMs, manage keys, retry logic, failover and limits in a central way
llm_providers:
  - access_key: $OPENAI_API_KEY
    model: openai/gpt-4o-mini
    default: true

# Arch creates a round-robin load balancing between different endpoints, managed via the cluster subsystem.
endpoints:
  weather_forecast_service:
    # value could be ip address or a hostname with port
    # this could also be a list of endpoints for load balancing
    # for example endpoint: [ ip1:port, ip2:port ]
    endpoint: host.docker.internal:18081
    # max time to wait for a connection to be established
    connect_timeout: 0.005s

# default system prompt used by all prompt targets
system_prompt: |
  You are a helpful weather assistant.

prompt_targets:
  - name: weather_forecast
    description: get the weather forecast
    parameters:
      - name: location
        description: the location for which to get the weather forecast
        required: true
        type: string
        format: City, State
      - name: days
        description: the number of days for the forecast
        required: true
        type: int
    endpoint:
      name: weather_forecast_service
      path: /weather
      http_method: POST

tracing:
  random_sampling: 100
  trace_arch_internal: true
